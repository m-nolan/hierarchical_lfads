{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LFADS Ecog prediction - Performance table\n",
    "Michael Nolan\n",
    "\n",
    "2020.10.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import site\n",
    "# site.addsitedir(os.path.curdir + '\\..')\n",
    "site.addsitedir(\"/home/ws5/manolan/ecog_pred/hierarchical_lfads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "def prep_model(model_name, data_dict, data_suffix, batch_size, device, hyperparams, input_dims=None):\n",
    "    if model_name == 'lfads_ecog':\n",
    "        # train_dl, valid_dl, input_dims, plotter = prep_data(data_dict=data_dict, data_suffix=data_suffix, batch_size=batch_size, device=device)\n",
    "        if not input_dims:\n",
    "            input_dims = data_dict['test_ecog_fl0u10'].shape[-1]\n",
    "        model, objective = prep_lfads_ecog(input_dims = input_dims,\n",
    "                                      hyperparams=hyperparams,\n",
    "                                      device= device,\n",
    "                                      dtype=data_dict['test_ecog_fl0u10'].dtype,\n",
    "                                      dt= data_dict['dt']\n",
    "                                      )\n",
    "    return model\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "def prep_lfads_ecog(input_dims, hyperparams, device, dtype, dt):\n",
    "    from objective import LFADS_Loss, LogLikelihoodGaussian\n",
    "    from lfads import LFADS_Ecog_SingleSession_Net\n",
    "\n",
    "    model = LFADS_Ecog_SingleSession_Net(input_size           = input_dims,\n",
    "                                    factor_size          = hyperparams['model']['factor_size'],\n",
    "                                    g_encoder_size       = hyperparams['model']['g_encoder_size'],\n",
    "                                    c_encoder_size       = hyperparams['model']['c_encoder_size'],\n",
    "                                    g_latent_size        = hyperparams['model']['g_latent_size'],\n",
    "                                    u_latent_size        = hyperparams['model']['u_latent_size'],\n",
    "                                    controller_size      = hyperparams['model']['controller_size'],\n",
    "                                    generator_size       = hyperparams['model']['generator_size'],\n",
    "                                    prior                = hyperparams['model']['prior'],\n",
    "                                    clip_val             = hyperparams['model']['clip_val'],\n",
    "                                    dropout              = hyperparams['model']['dropout'],\n",
    "                                    do_normalize_factors = hyperparams['model']['normalize_factors'],\n",
    "                                    max_norm             = hyperparams['model']['max_norm'],\n",
    "                                    device               = device).to(device)\n",
    "    \n",
    "    loglikelihood = LogLikelihoodGaussian()\n",
    "\n",
    "    objective = LFADS_Loss(loglikelihood            = loglikelihood,\n",
    "                           loss_weight_dict         = {'kl': hyperparams['objective']['kl'], \n",
    "                                                       'l2': hyperparams['objective']['l2']},\n",
    "                           l2_con_scale             = hyperparams['objective']['l2_con_scale'],\n",
    "                           l2_gen_scale             = hyperparams['objective']['l2_gen_scale']).to(device)\n",
    "\n",
    "    return model, objective\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------\n",
    "    \n",
    "def prep_data(data_dict, data_suffix, batch_size, device, input_size=None):\n",
    "    train_data  = torch.Tensor(data_dict['train_%s'%data_suffix])\n",
    "    valid_data  = torch.Tensor(data_dict['valid_%s'%data_suffix])\n",
    "    \n",
    "    num_trials, num_steps, _ = train_data.shape\n",
    "    if not input_size: # very hacky, I apologize to the lord\n",
    "        input_size = train_data.shape(-1)\n",
    "    \n",
    "    train_ds    = EcogTensorDataset(train_data,device=device)\n",
    "    valid_ds    = EcogTensorDataset(valid_data,device=device)\n",
    "    \n",
    "    train_dl    = torch.utils.data.DataLoader(train_ds, batch_size = batch_size, shuffle=True)\n",
    "    valid_dl    = torch.utils.data.DataLoader(valid_ds, batch_size = batch_size)\n",
    "    \n",
    "    TIME = torch._np.arange(0, num_steps*data_dict['dt'], data_dict['dt'])\n",
    "    \n",
    "    train_truth = {}\n",
    "    if 'train_rates' in data_dict.keys():\n",
    "        train_truth['rates'] = data_dict['train_rates']\n",
    "    if 'train_latent' in data_dict.keys():\n",
    "        train_truth['latent'] = data_dict['train_latent']\n",
    "    if 'valid_spikes' in data_dict.keys():\n",
    "        train_truth['spikes'] = data_dict['train_spikes']\n",
    "        \n",
    "    valid_truth = {}\n",
    "    if 'valid_rates' in data_dict.keys():\n",
    "        valid_truth['rates'] = data_dict['valid_rates']\n",
    "    if 'valid_latent' in data_dict.keys():\n",
    "        valid_truth['latent'] = data_dict['valid_latent']\n",
    "    if 'valid_spikes' in data_dict.keys():\n",
    "        valid_truth['spikes'] = data_dict['valid_spikes']\n",
    "\n",
    "    plotter = None\n",
    "    # plotter = {'train' : Plotter(time=TIME, truth=train_truth),\n",
    "    #            'valid' : Plotter(time=TIME, truth=valid_truth)}\n",
    "    \n",
    "    return train_dl, valid_dl, input_size, plotter\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "class EcogTensorDataset(Dataset):\n",
    "    r\"\"\"Dataset wrapping tensors.\n",
    "\n",
    "    Each sample will be retrieved by indexing tensors along the first dimension.\n",
    "\n",
    "    Arguments:\n",
    "        *tensors (Tensor): tensors that have the same size of the first dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *tensors, device='cpu'):\n",
    "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.device = device\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return tuple(tensor[index].to(self.device) for tensor in self.tensors)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "import h5py\n",
    "def read_data(data_fname,keys):\n",
    "    \n",
    "    \"\"\" Read saved data in HDF5 format.\n",
    "\n",
    "    Args:\n",
    "        data_fname: The filename of the file from which to read the data.\n",
    "    Returns:\n",
    "        A dictionary whose keys will vary depending on dataset (but should\n",
    "        always contain the keys 'train_data' and 'valid_data') and whose\n",
    "        values are numpy arrays.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with h5py.File(data_fname, 'r') as hf:\n",
    "            data_dict = {k: np.array(v) for k, v in hf.items() if k in keys}\n",
    "            return data_dict\n",
    "    except IOError:\n",
    "        print(\"Cannot open %s for reading.\" % data_fname)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prediction_metrics(test_data,pred,pred_window_T,bin_T,p_lim=[2.5, 97.5],srate=250):\n",
    "    n_trial, n_sample, n_ch = test_data.shape\n",
    "    time = np.arange(pred_window_T*srate)/srate\n",
    "    bin_T_left_edge = np.arange(pred_window_T,step=bin_T)\n",
    "    bin_T_right_edge = bin_T_left_edge + bin_T\n",
    "    n_time_bin = len(bin_T_left_edge)\n",
    "    mae, rmse, rpe = compute_prediction_error(test_data, pred)\n",
    "    mae_bin = np.empty((n_trial,n_time_bin))\n",
    "    rmse_bin = np.empty((n_trial,n_time_bin))\n",
    "    rpe_bin = np.empty((n_trial,n_time_bin))\n",
    "    for tb_idx in range(n_time_bin):\n",
    "        bin_idx = np.logical_and(time >= bin_T_left_edge[tb_idx], time < bin_T_right_edge[tb_idx])\n",
    "        mae_bin[:,tb_idx], rmse_bin[:,tb_idx], rpe_bin[:,tb_idx] = compute_prediction_error(test_data[:,bin_idx,:],pred[:,bin_idx,:])\n",
    "    # get stats from sample distributions\n",
    "    stat_dict = {\n",
    "        'mae_mean': mae.mean(),\n",
    "        'mae_95ci': np.percentile(mae,p_lim),\n",
    "        'mae_bin_mean': mae_bin.mean(axis=0),\n",
    "        'mae_bin_95ci': np.percentile(mae_bin,p_lim,axis=0),\n",
    "        'rmse_mean': rmse.mean(),\n",
    "        'rmse_95ci': np.percentile(rmse,p_lim),\n",
    "        'rmse_bin_mean': rmse_bin.mean(axis=0),\n",
    "        'rmse_bin_95ci': np.percentile(rmse_bin,p_lim,axis=0),\n",
    "        'rpe_mean': rpe.mean(),\n",
    "        'rpe_95ci': np.percentile(rpe,p_lim),\n",
    "        'rpe_bin_mean': np.nanmean(rpe_bin,axis=0),\n",
    "        'rpe_bin_95ci': np.nanpercentile(rpe_bin,p_lim,axis=0),\n",
    "        # 'corr_mean': np.tanh(np.arctanh(corr).mean(axis=0)),\n",
    "        # 'corr_95ci': np.percentile(corr,p_lim,axis=0),\n",
    "        # 'corr_bin_mean': np.tanh(np.arctanh(corr_bin).mean(axis=0)),\n",
    "        # 'corr_bin_95ci': np.percentile(corr_bin,p_lim,axis=0)\n",
    "    }\n",
    "    return stat_dict, bin_T_left_edge\n",
    "\n",
    "def compute_prediction_error(trg,pred):\n",
    "    err = trg - pred\n",
    "    mae = np.abs(err).mean(axis=(1,2))\n",
    "    rmse = np.sqrt((err**2).mean(axis=(1,2)))\n",
    "    trg_std = trg.std(axis=1)\n",
    "    rpe = (err.std(axis=1)/trg_std).mean(axis=-1)\n",
    "    rpe[np.isinf(rpe)] == np.nan\n",
    "    return mae, rmse, rpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter_path = \"C:\\\\Users\\\\mickey\\\\aoLab\\\\code\\\\hierarchical_lfads\\\\hyperparameters\\\\ecog\\\\lfads_ecog_3.yaml\"\n",
    "# data_path = \"D:\\\\Users\\\\mickey\\\\Data\\\\datasets\\\\ecog\\\\goose_wireless\\\\gw_250_fl0u10\"\n",
    "hyperparameter_path = \"/home/ws5/manolan/ecog_pred/hierarchical_lfads/hyperparameters/ecog/lfads_ecog_reduced.yaml\"\n",
    "data_path = \"/home/ws5/manolan/data/datasets/ecog/goose_wireless/gw_250_fl0u10\"\n",
    "data_suffix = \"ecog_fl0u10\"\n",
    "model_name = \"lfads_ecog\"\n",
    "batch_size = 1500\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-65565dbeffca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m  \u001b[0mload_parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mhyperparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhyperparameter_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata_dict\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'test_ecog_fl0u20'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'dt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m model= prep_model(model_name = model_name,\n\u001b[0;32m      5\u001b[0m                                         \u001b[0mdata_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "from utils import load_parameters\n",
    "hyperparams = load_parameters(hyperparameter_path)\n",
    "data_dict   = read_data(data_path,keys = ['test_ecog_fl0u20','dt'])\n",
    "model= prep_model(model_name = model_name,\n",
    "                                        data_dict = data_dict,\n",
    "                                        data_suffix = data_suffix,\n",
    "                                        batch_size = batch_size,\n",
    "                                        device = device,\n",
    "                                        hyperparams = hyperparams,\n",
    "                                        input_dims=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"D:\\\\Users\\\\mickey\\\\Data\\\\models\\\\pyt\\\\lfads\\\\gw_250_fl0u10\\\\lfads_ecog\\\\cenc0_cont0_fact64_genc128_gene128_glat128_ulat0_orion-\\\\checkpoints\\\\best.pth\"\n",
    "model_path = \"/home/ws5/manolan/data/models/pyt/lfads/gw_250_fl0u10/lfads_ecog/cenc0_cont0_fact64_genc128_gene128_glat128_nch8_seqlen130_ulat0_orion-/checkpoints/best.pth\"\n",
    "checkpoint = torch.load(model_path)\n",
    "model.load_state_dict(checkpoint['net'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_samples = data_dict['test_ecog_fl0u10'].shape[0]\n",
    "test_data = torch.tensor(data_dict['test_ecog_fl0u10'][:,:130,10:18]).float()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    recon, (factors, inputs) = model(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = np.mean((recon['data'].numpy() - test_data.numpy())**2,axis=(1,2))\n",
    "rmse = np.sqrt(mse)\n",
    "mae = np.abs(recon['data'].numpy() - test_data.numpy()).mean(axis=(1,2))\n",
    "trg_std = np.std(test_data.numpy(), axis=(1,2))\n",
    "trg_std[trg_std < 0.1] = np.nan\n",
    "rpe = rmse/trg_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(trg_std == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compute binned prediction metrics\n",
    "# stat_dict, bin_T_left_edge = compute_prediction_metrics(test_data,recon,1,0.1,srate=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {\n",
    "    'mse_mean': mse.mean(),\n",
    "    'mse_95CI': np.percentile(mse,[2.5,97.5]),\n",
    "    'rmse_mean': rmse.mean(),\n",
    "    'rmse_95CI': np.percentile(rmse,[2.5,97.5]),\n",
    "    'mae_mean': mae.mean(),\n",
    "    'mae_95CI': np.percentile(mae,[2.5,97.5]),\n",
    "    'rpe_mean': np.nanmean(rpe),\n",
    "    'rpe_95CI': np.nanpercentile(rmse,[2.5,97.5]),\n",
    "}\n",
    "print(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot_idx = np.random.randint(n_test_samples)\n",
    "# plot_idx = 6309\n",
    "# plt.plot(test_data[plot_idx,:,:],label='trg')\n",
    "# plt.plot(recon['data'][plot_idx,:,:],label='data')\n",
    "# # plt.plot(recon['rates'][:,plot_idx,:].detach().numpy(),label='rates')\n",
    "# plt.legend(loc=0)\n",
    "# print(trg_std[plot_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(3,1,figsize=(3,6),sharex=True,dpi=80)\n",
    "# # mae\n",
    "# ax[0].fill_between(bin_T_left_edge,stat_dict['mae_bin_95ci'][0,:],stat_dict['mae_bin_95ci'][1,:],alpha=0.2,label='95% CI')\n",
    "# ax[0].plot(bin_T_left_edge,stat_dict['mae_bin_mean'],label='mean')\n",
    "# ax[0].axhline(stat_dict['mae_mean'],color='k',linestyle=':',label='window mean')\n",
    "# ax[0].set_ylim(0,1.3)\n",
    "# ax[0].set_ylabel('MAE (a.u.)')\n",
    "# ax[0].set_title('MAE v. time')\n",
    "# ax[0].legend(loc=0)\n",
    "# # rmse\n",
    "# ax[1].fill_between(bin_T_left_edge,stat_dict['rmse_bin_95ci'][0,:],stat_dict['rmse_bin_95ci'][1,:],alpha=0.2,label='95% CI')\n",
    "# ax[1].plot(bin_T_left_edge,stat_dict['rmse_bin_mean'],label='mean')\n",
    "# ax[1].axhline(stat_dict['rmse_mean'],color='k',linestyle=':',label='window mean')\n",
    "# ax[1].set_ylim(0,1.3)\n",
    "# ax[1].set_ylabel('RMSE (a.u.)')\n",
    "# ax[1].set_title('RMSE v. time')\n",
    "# # rpe\n",
    "# ax[2].fill_between(bin_T_left_edge,stat_dict['rpe_bin_95ci'][0,:],stat_dict['rpe_bin_95ci'][1,:],alpha=0.2,label='95% CI')\n",
    "# ax[2].plot(bin_T_left_edge,stat_dict['rpe_bin_mean'],label='mean')\n",
    "# ax[2].axhline(stat_dict['rpe_mean'],color='k',linestyle=':',label='window mean')\n",
    "# ax[2].set_ylim(0,1.3)\n",
    "# ax[2].set_xlabel('time (s)')\n",
    "# ax[2].set_ylabel('RPE')\n",
    "# ax[2].set_title('RPE v. time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,4,dpi=80,figsize=(10,2),constrained_layout=True)\n",
    "ax[0].hist(rmse,100,alpha=0.5,density=True);\n",
    "ax[0].axvline(result_dict['rmse_mean'],color='k')\n",
    "ax[0].axvline(result_dict['rmse_95CI'][0],color='k',linestyle=':')\n",
    "ax[0].axvline(result_dict['rmse_95CI'][1],color='k',linestyle=':')\n",
    "ax[0].set_xlim(0,1.1*np.percentile(rmse,99))\n",
    "ax[0].set_title('RMSE')\n",
    "ax[1].hist(mse,100,alpha=0.5,density=True);\n",
    "ax[1].axvline(result_dict['mse_mean'],color='k')\n",
    "ax[1].axvline(result_dict['mse_95CI'][0],color='k',linestyle=':')\n",
    "ax[1].axvline(result_dict['mse_95CI'][1],color='k',linestyle=':')\n",
    "ax[1].set_xlim(0,1.1*np.percentile(mse,99))\n",
    "ax[1].set_title('MSE')\n",
    "ax[2].hist(mae,100,alpha=0.5,density=True);\n",
    "ax[2].axvline(result_dict['mae_mean'],color='k')\n",
    "ax[2].axvline(result_dict['mae_95CI'][0],color='k',linestyle=':')\n",
    "ax[2].axvline(result_dict['mae_95CI'][1],color='k',linestyle=':')\n",
    "ax[2].set_xlim(0,1.1*np.percentile(mae,99))\n",
    "ax[2].set_title('MAE')\n",
    "ax[3].hist(rpe,100,alpha=0.5,density=True,label='dist.');\n",
    "ax[3].axvline(result_dict['rpe_mean'],color='k',label='mean')\n",
    "ax[3].axvline(result_dict['rpe_95CI'][0],color='k',linestyle=':',label='95% c.i.')\n",
    "ax[3].axvline(result_dict['rpe_95CI'][1],color='k',linestyle=':')\n",
    "ax[3].axvline(1.0,color='r',label='1')\n",
    "ax[3].legend(loc=0)\n",
    "ax[3].set_xlim(0,1.1*np.max((np.nanpercentile(rpe,99),1.0)))\n",
    "ax[3].set_title('RPE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_idx = rpe[~np.isnan(rpe)].argmax()\n",
    "np.arange(n_test_samples)[~np.isnan(rpe)][masked_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecog_is2s",
   "language": "python",
   "name": "ecog_is2s"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
