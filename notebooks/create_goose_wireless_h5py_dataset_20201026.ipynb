{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.7 64-bit ('ecog_is2s': conda)",
   "display_name": "Python 3.7.7 64-bit ('ecog_is2s': conda)",
   "metadata": {
    "interpreter": {
     "hash": "fe8054fe0736511d0a995e424bd42fab5ba13013efdf79ed2907f82c79967e8d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# creating (and saving a single instance of) an h5py dataset for the hierarchical_lfads code (or any other time series reconstruction model I cook up or find from github)\n",
    "\n",
    "Michael Nolan\n",
    "\n",
    "2020.10.26"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import site\n",
    "from glob import glob\n",
    "site.addsitedir(\"C:\\\\Users\\\\mickey\\\\aoLab\\\\Code\\\\hierarchical_lfads\")\n",
    "from utils import write_data\n",
    "import pickle as pkl\n",
    "import h5py\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import aopy\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concat_dataset( src_t=1.0, trg_t=0.0, step_t=1.0, filt_str=''):\n",
    "    # get complete dataset and sampling arrays\n",
    "    data_path = \"C:\\\\Users\\\\mickey\\\\aoLab\\\\Data\\\\WirelessData\\\\Goose_Multiscale_M1\"\n",
    "    data_file_list = glob(os.path.join(data_path,f'18032[0-9]*\\\\0[0-9]*\\\\*ECOG_3.clfp_ds250{filt_str}.dat'))\n",
    "    print(f'files found:\\t{len(data_file_list)}')\n",
    "    # create dataset interface - may have been unnecessary...\n",
    "    src_t = 1.0\n",
    "    trg_t = 0.0\n",
    "    step_t = 1.0\n",
    "    df_list = [aopy.data.DataFile(dfp) for dfp in data_file_list]\n",
    "    dfds_list = [aopy.data.DatafileDataset(df,src_t,trg_t,step_t) for df in df_list]\n",
    "    dfcds = aopy.data.DatafileConcatDataset(dfds_list)\n",
    "    print(f'total samples: {len(dfcds)}')\n",
    "    return dfcds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ndarray_from_cds( cds, detrend=True, zscore=True ):\n",
    "    #create dataset from all samples after z-scoring and detrending\n",
    "    n_trial = len(cds)\n",
    "    n_t, n_ch = cds.__getitem__(0)[0].size()\n",
    "    data = np.empty((n_trial,n_t,n_ch))\n",
    "    empty_ch_count = np.empty(n_trial)\n",
    "    for trial_idx in tqdm(range(len(cds))):\n",
    "        _sample = cds.__getitem__(trial_idx)[0]\n",
    "        if detrend:\n",
    "            _sample = sp.signal.detrend(_sample,axis=0,type='linear')\n",
    "        if zscore:\n",
    "            _sample = sp.stats.zscore(_sample,axis=0)\n",
    "        empty_ch_count[trial_idx] = np.isnan(_sample.mean(axis=0)).sum()\n",
    "        data[trial_idx,:,:] = _sample\n",
    "    data = data[empty_ch_count == 0,:,:]\n",
    "    return data, empty_ch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_dict( data, dt, cds, shuffle=True, rng_seed=42, train_valid_test_split=(0.7,0.2,0.1), filt_str='' ):\n",
    "    # get split data indices\n",
    "    n_samples = data.shape[0]\n",
    "    n_train = round(n_samples*train_valid_test_split[0])\n",
    "    n_valid = round(n_samples*train_valid_test_split[1])\n",
    "    n_test = round(n_samples*train_valid_test_split[2])\n",
    "    if shuffle:\n",
    "        # shuffle your dataset trials\n",
    "        rng = np.random.default_rng(rng_seed)\n",
    "        data = rng.permutation(data,axis=0)\n",
    "    # create data dict\n",
    "    data_dict = {\n",
    "        f'train_ecog{filt_str}': data[:n_train,:,:],\n",
    "        f'valid_ecog{filt_str}': data[n_train:n_train+n_valid,:,:],\n",
    "        f'test_ecog{filt_str}': data[n_train+n_valid:,:,:],\n",
    "        'dt': dt, # does this need to be in here? could this be external?\n",
    "    }\n",
    "    param_dict = {\n",
    "        'file_list': [ds.datafile.data_file_path for ds in cds.datasets],\n",
    "        'src_t': cds.datasets[0].src_t,\n",
    "        'step_t': cds.datasets[0].step_t,\n",
    "        'rng_seed': rng_seed,\n",
    "        'train_valid_test_split': train_valid_test_split,\n",
    "    }\n",
    "    return data_dict, param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_str_list = ['','_fl0u10','_fl0u20','_fl0u30']\n",
    "for filt_str in filt_str_list:\n",
    "    cds = get_concat_dataset(filt_str = filt_str)\n",
    "    dt = 1/cds.srate\n",
    "    dataset, empty_count = create_ndarray_from_cds(cds)\n",
    "    data_dict, param_dict = create_data_dict(dataset, dt, cds, filt_str=filt_str)\n",
    "    data_dict.keys()\n",
    "    h5_dataset_dir = \"D:\\\\Users\\\\mickey\\\\Data\\\\datasets\\\\ecog\\\\goose_wireless\"\n",
    "    h5_dataset_path = os.path.join(h5_dataset_dir,f\"gw_250{filt_str}\")\n",
    "    write_data(h5_dataset_path,data_dict,compression=None)\n",
    "    param_file_path = os.path.join(h5_dataset_dir,f\"gw_250{filt_str}_param.pkl\")\n",
    "    with open(param_file_path,'wb') as param_f:\n",
    "        pkl.dump(param_dict,param_f)"
   ]
  }
 ]
}