{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('ecog_is2s': conda)",
   "metadata": {
    "interpreter": {
     "hash": "fe8054fe0736511d0a995e424bd42fab5ba13013efdf79ed2907f82c79967e8d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Generate performance table\n",
    "Michael Nolan\n",
    "\n",
    "2020.11.04"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.parallel.data_parallel import DataParallel\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import TABLEAU_COLORS\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "import site\n",
    "site.addsitedir(os.path.curdir + '\\..')\n",
    "from utils import load_parameters\n",
    "# site.addsitedir(\"/home/ws5/manolan/ecog_pred/hierarchical_lfads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "def prep_model(model_name, data_dict, data_suffix, batch_size, device, hyperparams, input_dims=None):\n",
    "    if model_name == 'lfads_ecog':\n",
    "        # train_dl, valid_dl, input_dims, plotter = prep_data(data_dict=data_dict, data_suffix=data_suffix, batch_size=batch_size, device=device)\n",
    "        if not input_dims:\n",
    "            input_dims = data_dict[f'test_{data_suffix}'].shape[-1]\n",
    "        model, objective = prep_lfads_ecog(input_dims = input_dims,\n",
    "                                      hyperparams=hyperparams,\n",
    "                                      device= device,\n",
    "                                      dtype=data_dict[f'test_{data_suffix}'].dtype,\n",
    "                                      dt= data_dict['dt']\n",
    "                                      )\n",
    "    elif model_name == 'lfads':\n",
    "        model, objective = prep_lfads(input_dims = input_dims,\n",
    "                                      hyperparams=hyperparams,\n",
    "                                      device= device,\n",
    "                                      dtype=data_dict[f'test_{data_suffix}'].dtype,\n",
    "                                      dt= data_dict['dt']\n",
    "                                      )\n",
    "    else:\n",
    "        None                  \n",
    "\n",
    "    return model\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "def prep_lfads_ecog(input_dims, hyperparams, device, dtype, dt):\n",
    "    from objective import LFADS_Loss, LogLikelihoodGaussian\n",
    "    from lfads import LFADS_Ecog_SingleSession_Net\n",
    "\n",
    "    model = LFADS_Ecog_SingleSession_Net(input_size           = input_dims,\n",
    "                                    factor_size          = hyperparams['model']['factor_size'],\n",
    "                                    g_encoder_size       = hyperparams['model']['g_encoder_size'],\n",
    "                                    c_encoder_size       = hyperparams['model']['c_encoder_size'],\n",
    "                                    g_latent_size        = hyperparams['model']['g_latent_size'],\n",
    "                                    u_latent_size        = hyperparams['model']['u_latent_size'],\n",
    "                                    controller_size      = hyperparams['model']['controller_size'],\n",
    "                                    generator_size       = hyperparams['model']['generator_size'],\n",
    "                                    prior                = hyperparams['model']['prior'],\n",
    "                                    clip_val             = hyperparams['model']['clip_val'],\n",
    "                                    dropout              = hyperparams['model']['dropout'],\n",
    "                                    do_normalize_factors = hyperparams['model']['normalize_factors'],\n",
    "                                    max_norm             = hyperparams['model']['max_norm'],\n",
    "                                    device               = device).to(device)\n",
    "    \n",
    "    loglikelihood = LogLikelihoodGaussian()\n",
    "\n",
    "    objective = LFADS_Loss(loglikelihood            = loglikelihood,\n",
    "                           loss_weight_dict         = {'kl': hyperparams['objective']['kl'], \n",
    "                                                       'l2': hyperparams['objective']['l2']},\n",
    "                           l2_con_scale             = hyperparams['objective']['l2_con_scale'],\n",
    "                           l2_gen_scale             = hyperparams['objective']['l2_gen_scale']).to(device)\n",
    "\n",
    "    return model, objective\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "def prep_lfads(input_dims, hyperparams, device, dtype, dt):\n",
    "    from objective import LFADS_Loss, LogLikelihoodGaussian\n",
    "    from lfads import LFADS_SingleSession_Net\n",
    "\n",
    "    model = LFADS_SingleSession_Net(input_size           = input_dims,\n",
    "                                    factor_size          = hyperparams['model']['factor_size'],\n",
    "                                    g_encoder_size       = hyperparams['model']['g_encoder_size'],\n",
    "                                    c_encoder_size       = hyperparams['model']['c_encoder_size'],\n",
    "                                    g_latent_size        = hyperparams['model']['g_latent_size'],\n",
    "                                    u_latent_size        = hyperparams['model']['u_latent_size'],\n",
    "                                    controller_size      = hyperparams['model']['controller_size'],\n",
    "                                    generator_size       = hyperparams['model']['generator_size'],\n",
    "                                    prior                = hyperparams['model']['prior'],\n",
    "                                    clip_val             = hyperparams['model']['clip_val'],\n",
    "                                    dropout              = hyperparams['model']['dropout'],\n",
    "                                    do_normalize_factors = hyperparams['model']['normalize_factors'],\n",
    "                                    max_norm             = hyperparams['model']['max_norm'],\n",
    "                                    device               = device).to(device)\n",
    "    \n",
    "    loglikelihood = LogLikelihoodGaussian()\n",
    "\n",
    "    objective = LFADS_Loss(loglikelihood            = loglikelihood,\n",
    "                           loss_weight_dict         = {'kl': hyperparams['objective']['kl'], \n",
    "                                                       'l2': hyperparams['objective']['l2']},\n",
    "                           l2_con_scale             = hyperparams['objective']['l2_con_scale'],\n",
    "                           l2_gen_scale             = hyperparams['objective']['l2_gen_scale']).to(device)\n",
    "\n",
    "    return model, objective\n",
    "#-------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------\n",
    "    \n",
    "def prep_data(data_dict, data_suffix, batch_size, device, input_size=None):\n",
    "    train_data  = torch.Tensor(data_dict['train_%s'%data_suffix])\n",
    "    valid_data  = torch.Tensor(data_dict['valid_%s'%data_suffix])\n",
    "    \n",
    "    num_trials, num_steps, _ = train_data.shape\n",
    "    if not input_size: # very hacky, I apologize to the lord\n",
    "        input_size = train_data.shape(-1)\n",
    "    \n",
    "    train_ds    = EcogTensorDataset(train_data,device=device)\n",
    "    valid_ds    = EcogTensorDataset(valid_data,device=device)\n",
    "    \n",
    "    train_dl    = torch.utils.data.DataLoader(train_ds, batch_size = batch_size, shuffle=True)\n",
    "    valid_dl    = torch.utils.data.DataLoader(valid_ds, batch_size = batch_size)\n",
    "    \n",
    "    TIME = torch._np.arange(0, num_steps*data_dict['dt'], data_dict['dt'])\n",
    "    \n",
    "    train_truth = {}\n",
    "    if 'train_rates' in data_dict.keys():\n",
    "        train_truth['rates'] = data_dict['train_rates']\n",
    "    if 'train_latent' in data_dict.keys():\n",
    "        train_truth['latent'] = data_dict['train_latent']\n",
    "    if 'valid_spikes' in data_dict.keys():\n",
    "        train_truth['spikes'] = data_dict['train_spikes']\n",
    "        \n",
    "    valid_truth = {}\n",
    "    if 'valid_rates' in data_dict.keys():\n",
    "        valid_truth['rates'] = data_dict['valid_rates']\n",
    "    if 'valid_latent' in data_dict.keys():\n",
    "        valid_truth['latent'] = data_dict['valid_latent']\n",
    "    if 'valid_spikes' in data_dict.keys():\n",
    "        valid_truth['spikes'] = data_dict['valid_spikes']\n",
    "\n",
    "    plotter = None\n",
    "    # plotter = {'train' : Plotter(time=TIME, truth=train_truth),\n",
    "    #            'valid' : Plotter(time=TIME, truth=valid_truth)}\n",
    "    \n",
    "    return train_dl, valid_dl, input_size, plotter\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "class EcogTensorDataset(Dataset):\n",
    "    r\"\"\"Dataset wrapping tensors.\n",
    "\n",
    "    Each sample will be retrieved by indexing tensors along the first dimension.\n",
    "\n",
    "    Arguments:\n",
    "        *tensors (Tensor): tensors that have the same size of the first dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *tensors, device='cpu'):\n",
    "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.device = device\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return tuple(tensor[index].to(self.device) for tensor in self.tensors)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "import h5py\n",
    "def read_data(data_fname,keys):\n",
    "    \n",
    "    \"\"\" Read saved data in HDF5 format.\n",
    "\n",
    "    Args:\n",
    "        data_fname: The filename of the file from which to read the data.\n",
    "    Returns:\n",
    "        A dictionary whose keys will vary depending on dataset (but should\n",
    "        always contain the keys 'train_data' and 'valid_data') and whose\n",
    "        values are numpy arrays.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with h5py.File(data_fname, 'r') as hf:\n",
    "            data_dict = {k: np.array(v) for k, v in hf.items() if k in keys}\n",
    "            return data_dict\n",
    "    except IOError:\n",
    "        print(\"Cannot open %s for reading.\" % data_fname)\n",
    "        raise\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "def compute_prediction_metrics(test_data,pred,pred_window_T,bin_T,p_lim=[2.5, 97.5],srate=250):\n",
    "    n_trial, n_sample, n_ch = test_data.shape\n",
    "    time = np.arange(pred_window_T*srate)/srate\n",
    "    bin_T_left_edge = np.arange(pred_window_T,step=bin_T)\n",
    "    bin_T_right_edge = bin_T_left_edge + bin_T\n",
    "    n_time_bin = len(bin_T_left_edge)\n",
    "    mae, rmse, rpe = compute_prediction_error(test_data, pred)\n",
    "    mae_bin = np.empty((n_trial,n_time_bin))\n",
    "    rmse_bin = np.empty((n_trial,n_time_bin))\n",
    "    rpe_bin = np.empty((n_trial,n_time_bin))\n",
    "    for tb_idx in range(n_time_bin):\n",
    "        bin_idx = np.logical_and(time >= bin_T_left_edge[tb_idx], time < bin_T_right_edge[tb_idx])\n",
    "        mae_bin[:,tb_idx], rmse_bin[:,tb_idx], rpe_bin[:,tb_idx] = compute_prediction_error(test_data[:,bin_idx,:],pred[:,bin_idx,:])\n",
    "    # get stats from sample distributions\n",
    "    stat_dict = {\n",
    "        'mae_mean': mae.mean(),\n",
    "        'mae_95ci': np.percentile(mae,p_lim),\n",
    "        'mae_bin_mean': mae_bin.mean(axis=0),\n",
    "        'mae_bin_95ci': np.percentile(mae_bin,p_lim,axis=0),\n",
    "        'rmse_mean': rmse.mean(),\n",
    "        'rmse_95ci': np.percentile(rmse,p_lim),\n",
    "        'rmse_bin_mean': rmse_bin.mean(axis=0),\n",
    "        'rmse_bin_95ci': np.percentile(rmse_bin,p_lim,axis=0),\n",
    "        'rpe_mean': rpe.mean(),\n",
    "        'rpe_95ci': np.percentile(rpe,p_lim),\n",
    "        'rpe_bin_mean': np.nanmean(rpe_bin,axis=0),\n",
    "        'rpe_bin_95ci': np.nanpercentile(rpe_bin,p_lim,axis=0),\n",
    "        # 'corr_mean': np.tanh(np.arctanh(corr).mean(axis=0)),\n",
    "        # 'corr_95ci': np.percentile(corr,p_lim,axis=0),\n",
    "        # 'corr_bin_mean': np.tanh(np.arctanh(corr_bin).mean(axis=0)),\n",
    "        # 'corr_bin_95ci': np.percentile(corr_bin,p_lim,axis=0)\n",
    "    }\n",
    "    return stat_dict, bin_T_left_edge\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "def compute_prediction_error(trg,pred):\n",
    "    err = trg - pred\n",
    "    mae = np.abs(err).mean(axis=(1,2))\n",
    "    rmse = np.sqrt((err**2).mean(axis=(1,2)))\n",
    "    trg_std = trg.std(axis=1)\n",
    "    rpe = (err.std(axis=1)/trg_std).mean(axis=-1)\n",
    "    rpe[np.isinf(rpe)] == np.nan\n",
    "    return mae, rmse, rpe\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "def bootstrap_est(data,n_boot,f):\n",
    "    n_sample = data.shape[0]\n",
    "    est = []\n",
    "    for n in range(n_boot):\n",
    "        _idx = np.random.choice(np.arange(n_sample),size=n_sample,replace=True)\n",
    "        est.append(f(data[_idx,]))\n",
    "    est = np.stack(est,axis=0)\n",
    "    return est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_params(model_dir_path):\n",
    "    # break down model_dir_path\n",
    "    model_key_list = model_dir_path.split(\"\\\\\")\n",
    "    conf_str = model_key_list[-1]\n",
    "    model_name = model_key_list[-2]\n",
    "    data_keys = model_key_list[-3].split(\"_\")\n",
    "    if len(data_keys) > 2:\n",
    "        data_suffix = 'ecog_' + data_keys[2]\n",
    "    else:\n",
    "        data_suffix = 'ecog'\n",
    "    # break down conf_str\n",
    "    conf_key_list = conf_str.split(\"_\") # what was this going to be used for?\n",
    "    return model_name, data_suffix\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "class DataParallelPassthrough(torch.nn.DataParallel):\n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super().__getattr__(name)\n",
    "        except AttributeError:\n",
    "            return getattr(self.module, name)\n",
    "    def __setattr__(self, name, value):\n",
    "        try:\n",
    "            return super().__setattr__(name,value)\n",
    "        except AttributeError:\n",
    "            return setattr(self.module, name, value)\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "def load_configure_model_data(model_dir_path,data_path,hyperparameter_path,std_thresh=0.5):\n",
    "    model_name, data_suffix = get_model_params(model_dir_path)\n",
    "    data_suffix = 'data'\n",
    "    batch_size = 1000\n",
    "    device = 'cpu'\n",
    "    hyperparams = load_parameters(hyperparameter_path)\n",
    "    data_dict   = read_data(data_path,keys = [f'test_{data_suffix}','dt','test_idx'])\n",
    "    if 'dt' not in data_dict.keys():\n",
    "        data_dict['dt'] = 0.01\n",
    "    n_trial, seq_len, n_ch = data_dict[f'test_{data_suffix}'].shape\n",
    "    model= prep_model(model_name = model_name,\n",
    "                                            data_dict = data_dict,\n",
    "                                            data_suffix = data_suffix,\n",
    "                                            batch_size = batch_size,\n",
    "                                            device = device,\n",
    "                                            hyperparams = hyperparams,\n",
    "                                            input_dims= n_ch)\n",
    "    checkpoint_file_path = os.path.join(model_dir_path,\"checkpoints\",\"best.pth\")\n",
    "    checkpoint = torch.load(checkpoint_file_path,map_location=device)\n",
    "    if list(checkpoint['net'].keys())[0][0:6] == 'module': # very dirty\n",
    "        model = DataParallelPassthrough(model)\n",
    "    model.load_state_dict(checkpoint['net'], strict=False)\n",
    "    if list(checkpoint['net'].keys())[0][0:6] == 'module':\n",
    "        model = model.module\n",
    "        model.g_posterior_mean = checkpoint['net']['module.g_posterior_mean']\n",
    "        model.g_posterior_logvar = checkpoint['net']['module.g_posterior_logvar']\n",
    "    # grab data\n",
    "    test_data = np.array(data_dict[f\"test_{data_suffix}\"],dtype=np.float32)\n",
    "    test_idx = data_dict['test_idx']\n",
    "    return model.to('cpu'), test_data, test_idx\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "def compute_model_outputs(model,test_data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        recon, (factors, generators, gen_inputs) = model.forward_all(test_data)\n",
    "    return recon, factors, generators, gen_inputs\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "def compute_metric_table(test_data,recon,model_dir_path):\n",
    "    n_trial, n_sample, n_ch = test_data.shape\n",
    "    model_name, data_suffix, n_ch, seq_len = get_model_params(model_dir_path)\n",
    "    # compute metrics\n",
    "    mse = np.mean((recon['data'].numpy() - test_data)**2,axis=(1,2))\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.abs(recon['data'].numpy() - test_data).mean(axis=(1,2))\n",
    "    trg_std = np.std(test_data, axis=(1,2))\n",
    "    rpe = rmse/trg_std\n",
    "    corr = np.array([np.corrcoef(t.T,r.T)[0,1] for t,r in zip(test_data.reshape(n_trial,-1),recon['data'].numpy().reshape(n_trial,-1))])\n",
    "    # compute statistics\n",
    "    f_est = lambda x: x.mean()\n",
    "    f_zcorr_est = lambda x: np.tanh(x).mean()\n",
    "    n_boot = 1000\n",
    "    mse_bsd = bootstrap_est(mse,n_boot,f_est)\n",
    "    rmse_bsd = bootstrap_est(rmse,n_boot,f_est)\n",
    "    mae_bsd = bootstrap_est(mae,n_boot,f_est)\n",
    "    rpe_bsd = bootstrap_est(rpe,n_boot,f_est)\n",
    "    zcorr_bsd = bootstrap_est(corr,n_boot,f_zcorr_est) # fisher transform for better stats\n",
    "    stat_dict = {\n",
    "        'model_path': model_dir_path,\n",
    "        'model_name': model_name,\n",
    "        'data_suffix': data_suffix,\n",
    "        'n_ch': n_ch,\n",
    "        'seq_len': seq_len,\n",
    "        'seq_t': seq_len/250, # whoopsie with the magic number\n",
    "        'mse_mean': [mse_bsd.mean()],\n",
    "        'mse_mean_2.5ci': [np.percentile(mse_bsd,2.5)],\n",
    "        'mse_mean_97.5ci': [np.percentile(mse_bsd,97.5)],\n",
    "        'mse_2.5ci': [np.percentile(mse,2.5)],\n",
    "        'mse_97.5ci': [np.percentile(mse,97.5)],\n",
    "        'rmse_mean': [rmse_bsd.mean()],\n",
    "        'rmse_mean_2.5ci': [np.percentile(rmse_bsd,2.5)],\n",
    "        'rmse_mean_97.5ci': [np.percentile(rmse_bsd,97.5)],\n",
    "        'rmse_2.5ci': [np.percentile(rmse,2.5)],\n",
    "        'rmse_97.5ci': [np.percentile(rmse,97.5)],\n",
    "        'mae_mean': [mae_bsd.mean()],\n",
    "        'mae_mean_2.5ci': [np.percentile(mae_bsd,2.5)],\n",
    "        'mae_mean_97.5ci': [np.percentile(mae_bsd,97.5)],\n",
    "        'mae_2.5ci': [np.percentile(mae,2.5)],\n",
    "        'mae_97.5ci': [np.percentile(mae,97.5)],\n",
    "        'rpe_mean': [rpe_bsd.mean()],\n",
    "        'rpe_mean_2.5ci': [np.percentile(rpe_bsd,2.5)],\n",
    "        'rpe_mean_97.5ci': [np.percentile(rpe_bsd,97.5)],\n",
    "        'rpe_2.5ci': [np.percentile(rpe,2.5)],\n",
    "        'rpe_97.5ci': [np.percentile(rpe,97.5)],\n",
    "        'corr_mean': [np.arctanh(zcorr_bsd.mean())],\n",
    "        'corr_mean_2.5ci': [np.arctanh(np.percentile(zcorr_bsd,2.5))],\n",
    "        'corr_mean_97.5ci': [np.arctanh(np.percentile(zcorr_bsd,97.5))],\n",
    "        'corr_2.5ci': [np.percentile(corr,2.5)],\n",
    "        'corr_97.5ci': [np.percentile(corr,97.5)],\n",
    "    }\n",
    "    stat_table = pd.DataFrame.from_dict(stat_dict)\n",
    "    metric_dict = {\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'rpe': rpe,\n",
    "        'corr': corr,\n",
    "    }\n",
    "    return stat_table, metric_dict\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "def get_model_performance_stat_table(model_dir_path,data_path,hyperparameter_path):\n",
    "    print(f'loading model from:\\t{model_dir_path}')\n",
    "    model, test_data = load_configure_model_data(model_dir_path,data_path,hyperparameter_path)\n",
    "    print('computing test data reconstructions...')\n",
    "    recon, factors, generators = compute_model_outputs(model,torch.tensor(test_data))\n",
    "    print('computing metric statistics...')\n",
    "    stat_table, metric_dict = compute_metric_table(test_data,recon,model_dir_path)\n",
    "    return stat_table, metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(model_dir):\n",
    "    # get loss curves\n",
    "    tensorboard_dir = os.path.join(model_dir,'tensorboard')\n",
    "    loss_dirs = glob(os.path.join(tensorboard_dir,'*_Loss_*'))\n",
    "    loss_dict = {}\n",
    "    for loss_dir in loss_dirs:\n",
    "        loss_base = os.path.basename(loss_dir)\n",
    "        ea = event_accumulator.EventAccumulator(os.path.join(tensorboard_dir,loss_dir))\n",
    "        ea.Reload()\n",
    "        loss_keys = ea.Tags()['scalars']\n",
    "        for loss_key in loss_keys:\n",
    "            _loss_data = np.array(ea.Scalars(loss_key))[:,-1]\n",
    "            loss_dict[loss_base] = _loss_data\n",
    "    loss_data = pd.DataFrame(loss_dict)\n",
    "    # plot loss curves\n",
    "    n_c = len(loss_dirs)\n",
    "    n_r = 1\n",
    "    fig, ax = plt.subplots(1,1,dpi=150,constrained_layout=True)\n",
    "    sns.lineplot(data=loss_data,ax=ax)\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.set_title('Training Loss')\n",
    "    return fig, loss_data\n",
    "\n",
    "def plot_test_data_fits(recon,test_data,n,srate,metrics):\n",
    "    # assume 1ch for now\n",
    "    n_trials = test_data.shape[0]\n",
    "    n_time = test_data.shape[1]\n",
    "    time = np.arange(n_time)/srate\n",
    "    trial_idx = np.random.choice(np.arange(n_trials),n,replace=False)\n",
    "    n_r = int(np.ceil(np.sqrt(n)))\n",
    "    n_c = int(np.ceil(n/n_r))\n",
    "    fig, ax = plt.subplots(n_r,n_c,dpi=150,constrained_layout=True,sharex=True,figsize=(6,6))\n",
    "    ax = ax.reshape(-1)\n",
    "    for idx, t_idx in enumerate(trial_idx):\n",
    "        r_idx = idx // n_c\n",
    "        c_idx = idx % n_c\n",
    "        ax[idx].plot(time,test_data[t_idx,:,0],label='target')\n",
    "        ax[idx].plot(time,recon['data'][t_idx,:,0],label='recon.')\n",
    "        if r_idx == n_r - 1:\n",
    "            ax[idx].set_xlabel('time (s)')\n",
    "        if c_idx == 0:\n",
    "            ax[idx].set_ylabel('a.u.')\n",
    "        ax[idx].set_title(f'trial {t_idx}')\n",
    "        metric_str = f\"mse: {metrics['mse'][t_idx]:0.3f}\\nrpe: {metrics['rpe'][t_idx]:0.3f}\\ncorr: {metrics['corr'][t_idx]:0.3f}\"\n",
    "        text_x = ax[idx].get_xlim()[0]\n",
    "        text_y = ax[idx].get_ylim()[0]\n",
    "        ax[idx].text(text_x,text_y,metric_str,horizontalalignment='left',verticalalignment='bottom',fontsize=8,bbox=dict(alpha=0.1))\n",
    "    ax[0].legend(loc=0)\n",
    "    return fig, ax\n",
    "\n",
    "def plot_test_data_fits_psd(recon, test_data, srate, n_boot):\n",
    "    # power features\n",
    "    from scipy.signal import welch, detrend\n",
    "    trial_mask = test_data[:,:,0].std(axis=1) < 0.5\n",
    "    f_psd, data_psd = welch(detrend(test_data[~trial_mask,],type='linear',axis=-2),fs=250,axis=1)\n",
    "    _, recon_psd = welch(detrend(recon['data'][~trial_mask,],type='linear',axis=-2),fs=250,axis=1)\n",
    "    f_est = lambda x: x.mean(axis=0)\n",
    "    data_psd_bsd = bootstrap_est(data_psd[:,:,0], n_boot, f_est)\n",
    "    data_psd_mean = data_psd_bsd.mean(axis=0)\n",
    "    data_psd_95ci = np.percentile(data_psd_bsd,[2.5, 97.5],axis=0)\n",
    "    recon_psd_bsd = bootstrap_est(recon_psd[:,:,0], n_boot, f_est)\n",
    "    recon_psd_mean = recon_psd_bsd.mean(axis=0)\n",
    "    recon_psd_95ci = np.percentile(recon_psd_bsd,[2.5, 97.5],axis=0)\n",
    "    # diff_psd = 10*np.log10(recon_psd[:,:,0])-10*np.log10(data_psd[:,:,0])\n",
    "    diff_psd = recon_psd[:,:,0]/data_psd[:,:,0]\n",
    "    diff_psd_bsd = bootstrap_est(diff_psd,n_boot,f_est)\n",
    "    diff_psd_mean = diff_psd_bsd.mean(axis=0)\n",
    "    diff_psd_95ci = np.percentile(diff_psd_bsd,[2.5, 97.5],axis=0)\n",
    "    fig, ax = plt.subplots(1,1,dpi=100,sharex=True)\n",
    "    ax.fill_between(f_psd, 10*np.log10(data_psd_95ci[0,:]), 10*np.log10(data_psd_95ci[1,:]),alpha=0.2,label='data 95% ci')\n",
    "    ax.plot(f_psd, 10*np.log10(data_psd_mean), label='data mean');\n",
    "    ax.fill_between(f_psd, 10*np.log10(recon_psd_95ci[0,:]), 10*np.log10(recon_psd_95ci[1,:]),alpha=0.2,label='recon. 95% ci')\n",
    "    ax.plot(f_psd, 10*np.log10(recon_psd_mean), label='recon. mean');\n",
    "    ax.legend(loc=0)\n",
    "    # ax[1].fill_between(f_psd, 10*np.log10(diff_psd_95ci[0,:]), 10*np.log10(diff_psd_95ci[1,:]), color='k', alpha=0.2, label='diff 95% ci')\n",
    "    # ax[1].plot(f_psd, 10*np.log10(diff_psd_mean), color='k', label='diff. mean')\n",
    "    # ax[1].legend(loc=0)\n",
    "    ax.set_xlabel('freq. (Hz)')\n",
    "    ax.set_ylabel('PSD (dBu)')\n",
    "    ax.set_title('Power Spectral Density, Data v. Reconstruction')\n",
    "    return fig, ax\n",
    "\n",
    "def model_visualization(model_dir_path,data_path,hyperparameter_path,n,srate,n_boot,metrics):\n",
    "    print(f'loading model from:\\t{model_dir_path}')\n",
    "    model, test_data = load_configure_model_data(model_dir_path,data_path,hyperparameter_path)\n",
    "    print('computing test data reconstructions...')\n",
    "    recon, factors, generators = compute_model_outputs(model,torch.tensor(test_data))\n",
    "    print('Generating test plots...')\n",
    "    f_trace, _ = plot_test_data_fits(recon, test_data, n, srate, metrics)\n",
    "    f_psd, _ = plot_test_data_fits_psd(recon, test_data, srate, n_boot)\n",
    "    return f_trace, f_psd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"D:\\\\Users\\\\mickey\\\\Data\\\\datasets\\\\ecephys\\\\session_756029989\\\\static_gratings\"\n",
    "trial_data_path = \"D:\\\\Users\\\\mickey\\\\Data\\\\datasets\\\\ecephys\\\\session_756029989\\\\static_gratings.csv\"\n",
    "overwrite = False\n",
    "assert os.path.exists(data_path), \"data file not found.\"\n",
    "model_dir = \"D:\\\\Users\\\\mickey\\\\Data\\\\models\\\\pyt\\\\lfads\\\\static_gratings\\\\lfads\"\n",
    "# model_dir_list = glob(os.path.join(model_dir,\"cenc0_cont0_fact16_genc128_gene200_glat64_ulat0_orion-\")) # no controller\n",
    "model_dir_list = glob(os.path.join(model_dir,\"cenc64_cont100_fact16_genc128_gene200_glat64_ulat1_orion-\")) # 1-d controller\n",
    "print(f\"unassessed models found:\\t{len(model_dir_list)}\")\n",
    "print(model_dir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir_path = model_dir_list[0]\n",
    "hyperparameter_path = os.path.join(model_dir_path,'hyperparameters.yaml')\n",
    "trial_data = pd.read_csv(trial_data_path)\n",
    "model, test_data, test_idx = load_configure_model_data(model_dir_path,data_path,hyperparameter_path)\n",
    "test_trial_data = trial_data.iloc[test_idx,:]\n",
    "recon, factors, generators, inputs = compute_model_outputs(model,torch.tensor(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orientations = test_trial_data.orientation.unique()\n",
    "orientations = np.sort(orientations[~np.isnan(orientations)])\n",
    "time = np.arange(-0.1,0.35,0.01)\n",
    "v_min = 0\n",
    "v_max = 3\n",
    "f_hist, ax_hist = plt.subplots(2,3,figsize=(8,4),dpi=150,constrained_layout=True,sharex=True,sharey=True)\n",
    "ax_hist = ax_hist.reshape(-1)\n",
    "for or_idx, orientation in enumerate(orientations):\n",
    "    pcm = ax_hist[or_idx].pcolormesh(\n",
    "        time,\n",
    "        np.arange(test_data.shape[-1]),\n",
    "        test_data[test_trial_data.orientation==orientation].mean(axis=0).T,\n",
    "        vmin=v_min,\n",
    "        vmax=v_max,\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    ax_hist[or_idx].axvline(0.0,color='g')\n",
    "    ax_hist[or_idx].axvline(0.25,color='r')\n",
    "    ax_hist[or_idx].set_title(f'${orientation}^o$')\n",
    "[ax.set_xlabel('time (s)') for ax in ax_hist[-3:]]\n",
    "ax_hist[0].set_ylabel('unit')\n",
    "ax_hist[3].set_ylabel('unit')\n",
    "f_hist.savefig('G:\\\\My Drive\\\\Courses\\\\CSE 599B\\\\final_report\\\\report\\\\figures\\\\orientation_hgram.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = test_trial_data.spatial_frequency.unique()\n",
    "frequencies = np.sort(frequencies[~np.isnan(frequencies)])\n",
    "time = np.arange(-0.1,0.35,0.01)\n",
    "v_min = 0\n",
    "v_max = 3\n",
    "f_hist, ax_hist = plt.subplots(2,3,figsize=(8,4),dpi=150,constrained_layout=True,sharex=True,sharey=True)\n",
    "ax_hist = ax_hist.reshape(-1)\n",
    "for or_idx, frequency in enumerate(frequencies):\n",
    "    pcm = ax_hist[or_idx].pcolormesh(\n",
    "        time,\n",
    "        np.arange(test_data.shape[-1]),\n",
    "        test_data[test_trial_data.spatial_frequency==frequency].mean(axis=0).T,\n",
    "        vmin=v_min,\n",
    "        vmax=v_max,\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    ax_hist[or_idx].axvline(0.0,color='g')\n",
    "    ax_hist[or_idx].axvline(0.25,color='r')\n",
    "    ax_hist[or_idx].set_title(f'{frequency} cycle/deg.')\n",
    "[ax.set_xlabel('time (s)') for ax in ax_hist[-3:]]\n",
    "ax_hist[0].set_ylabel('unit')\n",
    "ax_hist[3].set_ylabel('unit')\n",
    "ax_hist[-1].set_axis_off()\n",
    "f_hist.savefig('G:\\\\My Drive\\\\Courses\\\\CSE 599B\\\\final_report\\\\report\\\\figures\\\\frequency_hgram.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases = test_trial_data.phase.unique()\n",
    "phases = np.sort(phases[~np.isnan(phases)])\n",
    "time = np.arange(-0.1,0.35,0.01)\n",
    "v_min = 0\n",
    "v_max = 3\n",
    "f_hist, ax_hist = plt.subplots(2,2,figsize=(5.5,4),dpi=150,constrained_layout=True,sharex=True,sharey=True)\n",
    "ax_hist = ax_hist.reshape(-1)\n",
    "for or_idx, phase in enumerate(phases):\n",
    "    pcm = ax_hist[or_idx].pcolormesh(\n",
    "        time,\n",
    "        np.arange(test_data.shape[-1]),\n",
    "        test_data[test_trial_data.phase==phase].mean(axis=0).T,\n",
    "        vmin=v_min,\n",
    "        vmax=v_max,\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    ax_hist[or_idx].axvline(0.0,color='g')\n",
    "    ax_hist[or_idx].axvline(0.25,color='r')\n",
    "    ax_hist[or_idx].set_title(f'{phase}')\n",
    "[ax.set_xlabel('time (s)') for ax in ax_hist[-2:]]\n",
    "ax_hist[0].set_ylabel('unit')\n",
    "ax_hist[2].set_ylabel('unit')\n",
    "f_hist.savefig('G:\\\\My Drive\\\\Courses\\\\CSE 599B\\\\final_report\\\\report\\\\figures\\\\phase_hgram.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_boot = 1000\n",
    "f_est = lambda x: x.mean(axis=0)\n",
    "f_factor,ax_factor = plt.subplots(4,4,figsize=(8,4),dpi=150,constrained_layout=True,sharex=True)\n",
    "f_input,ax_input =  plt.subplots(1,1,dpi=150,constrained_layout=True)\n",
    "ax_factor = ax_factor.reshape(-1)\n",
    "time = np.arange(-0.1,0.35,0.01)\n",
    "orientations = test_trial_data.orientation.unique()\n",
    "orientations = np.sort(orientations[~np.isnan(orientations)])\n",
    "colors = list(TABLEAU_COLORS)\n",
    "n_sample, n_trial, n_factor = factors.shape\n",
    "for or_idx, orientation in enumerate(orientations):\n",
    "    samples = factors[:,np.array(test_trial_data.orientation == orientation),:]\n",
    "    for factor_idx in range(n_factor):\n",
    "        # ax[factor_idx].plot(time,samples[:,:,factor_idx],color=TABLEAU_COLORS[colors[or_idx]],alpha=0.1);\n",
    "        trace_bs_dist = bootstrap_est(samples[:,:,factor_idx].T, n_boot, f_est)\n",
    "        mean_trace = trace_bs_dist.mean(axis=0)\n",
    "        ci_trace = np.percentile(trace_bs_dist,[2.5, 97.5],axis=0)\n",
    "        ax_factor[factor_idx].fill_between(time,ci_trace[0,:],ci_trace[1,:],color=TABLEAU_COLORS[colors[or_idx]],alpha=0.2)\n",
    "        ax_factor[factor_idx].plot(time,mean_trace,color=TABLEAU_COLORS[colors[or_idx]],label=str(orientation))\n",
    "        ax_factor[factor_idx].axvline(0.0,color='k',linestyle='--')\n",
    "        ax_factor[factor_idx].axvline(0.25,color='k',linestyle=':')\n",
    "        ax_factor[factor_idx].text(.99,0.01,f'{factor_idx}',horizontalalignment='right',verticalalignment='bottom',transform=ax_factor[factor_idx].transAxes)\n",
    "    if inputs is not None:\n",
    "        input_bs_dist = bootstrap_est(inputs[:,np.array(test_trial_data.orientation == orientation),0].T,n_boot,f_est)\n",
    "        mean_input = input_bs_dist.mean(axis=0)\n",
    "        ci_input = np.percentile(input_bs_dist,[2.5,97.5],axis=0)\n",
    "        ax_input.fill_between(time,ci_input[0,:],ci_input[1,:],color=TABLEAU_COLORS[colors[or_idx]],alpha=0.2)\n",
    "        ax_input.plot(time,mean_input,color=TABLEAU_COLORS[colors[or_idx]],label=str(orientation))\n",
    "        ax_input.axvline(0.0,color='k',linestyle='--')\n",
    "        ax_input.axvline(0.25,color='k',linestyle=':')\n",
    "\n",
    "ax_factor[3].legend(loc=\"upper left\",bbox_to_anchor=(1.01,1.0),frameon=False,ncol=2)\n",
    "[ax.set_xlabel('time (s)') for ax in ax_factor[-4:]];\n",
    "\n",
    "ax_input.legend(loc=0)\n",
    "ax_input.set_xlabel('time (s)')\n",
    "ax_input.set_title('LFADS Input (Orientation)')\n",
    "\n",
    "f_factor.savefig('G:\\\\My Drive\\\\Courses\\\\CSE 599B\\\\final_report\\\\report\\\\figures\\\\orientation_factor_u1.png')\n",
    "f_input.savefig('G:\\\\My Drive\\\\Courses\\\\CSE 599B\\\\final_report\\\\report\\\\figures\\\\orientation_input_u1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_boot = 1000\n",
    "f_est = lambda x: x.mean(axis=0)\n",
    "f_factor,ax_factor = plt.subplots(4,4,figsize=(8,4),dpi=150,constrained_layout=True,sharex=True)\n",
    "f_input,ax_input =  plt.subplots(1,1,dpi=150,constrained_layout=True)\n",
    "ax_factor = ax_factor.reshape(-1)\n",
    "time = np.arange(-0.1,0.35,0.01)\n",
    "frequencies = test_trial_data.spatial_frequency.unique()\n",
    "frequencies = np.sort(frequencies[~np.isnan(frequencies)])\n",
    "colors = list(TABLEAU_COLORS)\n",
    "n_sample, n_trial, n_factor = factors.shape\n",
    "for or_idx, orientation in enumerate(frequencies):\n",
    "    samples = factors[:,np.array(test_trial_data.spatial_frequency == orientation),:]\n",
    "    for factor_idx in range(n_factor):\n",
    "        # ax[factor_idx].plot(time,samples[:,:,factor_idx],color=TABLEAU_COLORS[colors[or_idx]],alpha=0.1);\n",
    "        trace_bs_dist = bootstrap_est(samples[:,:,factor_idx].T, n_boot, f_est)\n",
    "        mean_trace = trace_bs_dist.mean(axis=0)\n",
    "        ci_trace = np.percentile(trace_bs_dist,[2.5, 97.5],axis=0)\n",
    "        ax_factor[factor_idx].fill_between(time,ci_trace[0,:],ci_trace[1,:],color=TABLEAU_COLORS[colors[or_idx]],alpha=0.2)\n",
    "        ax_factor[factor_idx].plot(time,mean_trace,color=TABLEAU_COLORS[colors[or_idx]],label=str(orientation))\n",
    "        ax_factor[factor_idx].axvline(0.0,color='k',linestyle='--')\n",
    "        ax_factor[factor_idx].axvline(0.25,color='k',linestyle=':')\n",
    "        ax_factor[factor_idx].text(.99,0.01,f'{factor_idx}',horizontalalignment='right',verticalalignment='bottom',transform=ax_factor[factor_idx].transAxes)\n",
    "    if inputs is not None:\n",
    "        input_bs_dist = bootstrap_est(inputs[:,np.array(test_trial_data.spatial_frequency == orientation),0].T,n_boot,f_est)\n",
    "        mean_input = input_bs_dist.mean(axis=0)\n",
    "        ci_input = np.percentile(input_bs_dist,[2.5,97.5],axis=0)\n",
    "        ax_input.fill_between(time,ci_input[0,:],ci_input[1,:],color=TABLEAU_COLORS[colors[or_idx]],alpha=0.2)\n",
    "        ax_input.plot(time,mean_input,color=TABLEAU_COLORS[colors[or_idx]],label=str(orientation))\n",
    "        ax_input.axvline(0.0,color='k',linestyle='--')\n",
    "        ax_input.axvline(0.25,color='k',linestyle=':')\n",
    "\n",
    "ax_factor[3].legend(loc=\"upper left\",bbox_to_anchor=(1.01,1.0),frameon=False,ncol=2)\n",
    "[ax.set_xlabel('time (s)') for ax in ax_factor[-4:]];\n",
    "\n",
    "ax_input.legend(loc=0)\n",
    "ax_input.set_xlabel('time (s)')\n",
    "ax_input.set_title('LFADS Input (Spatial Freq.)')\n",
    "\n",
    "f_factor.savefig('G:\\\\My Drive\\\\Courses\\\\CSE 599B\\\\final_report\\\\report\\\\figures\\\\frequency_factor_u1.png')\n",
    "f_input.savefig('G:\\\\My Drive\\\\Courses\\\\CSE 599B\\\\final_report\\\\report\\\\figures\\\\frequency_input_u1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_boot = 1000\n",
    "f_est = lambda x: x.mean(axis=0)\n",
    "f_factor,ax_factor = plt.subplots(4,4,figsize=(8,4),dpi=150,constrained_layout=True,sharex=True)\n",
    "f_input,ax_input =  plt.subplots(1,1,dpi=150,constrained_layout=True)\n",
    "ax_factor = ax_factor.reshape(-1)\n",
    "time = np.arange(-0.1,0.35,0.01)\n",
    "phases = test_trial_data.phase.unique()\n",
    "phases = np.sort(phases[~np.isnan(phases)])\n",
    "colors = list(TABLEAU_COLORS)\n",
    "n_sample, n_trial, n_factor = factors.shape\n",
    "for or_idx, orientation in enumerate(phases):\n",
    "    samples = factors[:,np.array(test_trial_data.phase == orientation),:]\n",
    "    for factor_idx in range(n_factor):\n",
    "        # ax[factor_idx].plot(time,samples[:,:,factor_idx],color=TABLEAU_COLORS[colors[or_idx]],alpha=0.1);\n",
    "        trace_bs_dist = bootstrap_est(samples[:,:,factor_idx].T, n_boot, f_est)\n",
    "        mean_trace = trace_bs_dist.mean(axis=0)\n",
    "        ci_trace = np.percentile(trace_bs_dist,[2.5, 97.5],axis=0)\n",
    "        ax_factor[factor_idx].fill_between(time,ci_trace[0,:],ci_trace[1,:],color=TABLEAU_COLORS[colors[or_idx]],alpha=0.2)\n",
    "        ax_factor[factor_idx].plot(time,mean_trace,color=TABLEAU_COLORS[colors[or_idx]],label=str(orientation))\n",
    "        ax_factor[factor_idx].axvline(0.0,color='k',linestyle='--')\n",
    "        ax_factor[factor_idx].axvline(0.25,color='k',linestyle=':')\n",
    "        ax_factor[factor_idx].text(.99,0.01,f'{factor_idx}',horizontalalignment='right',verticalalignment='bottom',transform=ax_factor[factor_idx].transAxes)\n",
    "    if inputs is not None:\n",
    "        input_bs_dist = bootstrap_est(inputs[:,np.array(test_trial_data.phase == orientation),0].T,n_boot,f_est)\n",
    "        mean_input = input_bs_dist.mean(axis=0)\n",
    "        ci_input = np.percentile(input_bs_dist,[2.5,97.5],axis=0)\n",
    "        ax_input.fill_between(time,ci_input[0,:],ci_input[1,:],color=TABLEAU_COLORS[colors[or_idx]],alpha=0.2)\n",
    "        ax_input.plot(time,mean_input,color=TABLEAU_COLORS[colors[or_idx]],label=str(orientation))\n",
    "        ax_input.axvline(0.0,color='k',linestyle='--')\n",
    "        ax_input.axvline(0.25,color='k',linestyle=':')\n",
    "\n",
    "ax_factor[3].legend(loc=\"upper left\",bbox_to_anchor=(1.01,1.0),frameon=False,ncol=2)\n",
    "[ax.set_xlabel('time (s)') for ax in ax_factor[-4:]];\n",
    "\n",
    "ax_input.legend(loc=0)\n",
    "ax_input.set_xlabel('time (s)')\n",
    "ax_input.set_title('LFADS Input (Phase)')\n",
    "\n",
    "f_factor.savefig('G:\\\\My Drive\\\\Courses\\\\CSE 599B\\\\final_report\\\\report\\\\figures\\\\phase_factor_u1.png')\n",
    "f_input.savefig('G:\\\\My Drive\\\\Courses\\\\CSE 599B\\\\final_report\\\\report\\\\figures\\\\phase_input_u1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "def plot_loss_curves(model_dir):\n",
    "    # get loss curves\n",
    "    tensorboard_dir = os.path.join(model_dir,'tensorboard')\n",
    "    loss_dirs = glob(os.path.join(tensorboard_dir,'*_Loss_*'))\n",
    "    loss_dict = {}\n",
    "    for loss_dir in loss_dirs:\n",
    "        loss_base = os.path.basename(loss_dir)\n",
    "        ea = event_accumulator.EventAccumulator(os.path.join(tensorboard_dir,loss_dir))\n",
    "        ea.Reload()\n",
    "        loss_keys = ea.Tags()['scalars']\n",
    "        for loss_key in loss_keys:\n",
    "            _loss_data = np.array(ea.Scalars(loss_key))[:,-1]\n",
    "            loss_dict[loss_base] = _loss_data\n",
    "    loss_data = pd.DataFrame(loss_dict)\n",
    "    # plot loss curves\n",
    "    n_c = len(loss_dirs)\n",
    "    n_r = 1\n",
    "    fig, ax = plt.subplots(1,1,dpi=150,constrained_layout=True)\n",
    "    sns.lineplot(data=loss_data,ax=ax)\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.set_title('Training Loss')\n",
    "    return fig, loss_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u1_loss_fig, u1_loss_data = plot_loss_curves(model_dir_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_loss, ax_loss = plt.subplots(2,1,dpi=150,sharex=True)\n",
    "ax_loss[0].plot(u1_loss_data['1_Loss_3_total_Training'],label='training loss')\n",
    "ax_loss[0].plot(u1_loss_data['1_Loss_3_total_Validation'],label='validation loss')\n",
    "ax_loss[1].plot(u1_loss_data['1_Loss_3_total_Training'],label='training loss')\n",
    "ax_loss[1].text(1500,u1_loss_data['1_Loss_3_total_Training'].values[-1]+50,f\"train: {u1_loss_data['1_Loss_3_total_Training'].values[-1]:0.2f}\",horizontalalignment='right',verticalalignment='bottom')\n",
    "ax_loss[1].plot(u1_loss_data['1_Loss_3_total_Validation'],label='validation loss')\n",
    "ax_loss[1].text(1500,u1_loss_data['1_Loss_3_total_Validation'].values[-1]-50,f\"valid.: {u1_loss_data['1_Loss_3_total_Validation'].values[-1]:0.2f}\",horizontalalignment='right',verticalalignment='top')\n",
    "ax_loss[1].set_ylim(7500,8500)\n",
    "ax_loss[1].set_xlabel('epoch')\n",
    "ax_loss[0].set_ylabel('loss obj.')\n",
    "ax_loss[1].set_ylabel('loss obj.')\n",
    "ax_loss[0].set_title('Homogeneous LFADS Learning Curve')\n",
    "ax_loss[1].legend(loc=0)\n",
    "f_loss.savefig('G:\\\\My Drive\\\\Courses\\\\CSE 599B\\\\final_report\\\\report\\\\figures\\\\learning_curve_u0.png')"
   ]
  }
 ]
}